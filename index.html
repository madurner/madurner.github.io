<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>MDurner</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
        <!-- other icons -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Maximilian Durner</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/01aa422427c66b9614cdf79dd3cbf4cc.png" alt="" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#experience">Experience</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#projects">Projects</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <!--<li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Acomplishments</a></li>-->
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Maximilian
                        <span class="text-primary">Durner</span>
                    </h1>
                    <div class="subheading mb-5">
                        <!--Test Street · Cheyenne Wells, CO 80810 · (317) 585-8468 ·-->
                        Enthusiastic in the field of Computer Vision, Machine Learning and Robotics
                        <br/>
                        <a href="mailto:name@email.com">max.durner@gmail.com</a>
                    </div>
                    <p class="lead mb-5">I am researcher at the Department of Perception and Cognition at the Institute of Robotics and Mechatronics, German Aerospace Center (DLR). My current research interests include Computer Vision, Artificial Intelligence, and Robotics.</p>
                    <div class="social-icons">
                        <a class="social-icon" href="https://www.linkedin.com/in/maximilian-durner-a44489112/"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" href="https://github.com/madurner/"><i class="fab fa-github"></i></a>
                        <a class="social-icon" href="https://scholar.google.com/citations?user=3KIopFIAAAAJ&hl=de"><i class="ai ai-google-scholar"></i></a>
                        <a class="social-icon" href="assets/2020_cv_eng_academic.pdf"><i class="fas fa-id-card"></i></a>
                              <!--
                              <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="far fa-file fa-stack-1x fa-inverse"></i>
                              </span>
                                -->
                        <!--<a class="social-icon" href="#"><i class="fab fa-twitter"></i></a>-->
                        <!--<a class="social-icon" href="#"><i class="fab fa-facebook-f"></i></a>-->
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Experience-->
            <section class="resume-section" id="experience">
                <div class="resume-section-content">
                    <h2 class="mb-5">Experience</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-3">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Teaching Assistant: <a href="https://vision.in.tum.de/teaching/ws2020/ml4cv">Machine Learning for Computer Vision</a> </h3>
                            <div class="subheading">Chair of Computer Vision & Artificial Intelligence, Technical University of Munich</div>
                            <!--<p>Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.</p>-->
                            <ul class="fa-ul mb-0">
                            <li>
                                <span class="fa-li"><i class="fas fa-angle-right"></i></span>
                                Teaching course related exercises twice a week (~40 students)
                            </li>
                            <li>
                                <span class="fa-li"><i class="fa fa-angle-right" aria-hidden="true"></i></span>
                                Lessons focus on: bayesian statistics, classical machine learning, deep learning (theoretical and practical)
                            </li>
                            </ul>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">September 2019 - Present</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-3">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Research Associate in Computer Vision and Machine Learning</h3>
                            <div class="subheading mb-3">German Aerospace Center, <a href="https://www.dlr.de/rmc/"> Institute of Robotics and Mechatronics </a> </div>
                            <ul class="fa-ul mb-0">
                            <li>
                                <span class="fa-li"><i class="fa fa-angle-right"></i></span>
                                Research on class-agnostic object segmentation, scalable learning approaches for object related vision algorithms, fusion of robotic vision and manipulation
                            </li>
                            <li>
                                <span class="fa-li"><i class="fa fa-angle-right"></i></span>
                                Implementation of algorithms and integration of visual sensors for mobile robots
                            </li>
                            </ul>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">May 2016 - Present</span></div>
                    </div>
                    <div>
                        <h3 class="mb-0">Working Student</h3>
                        <div class="mb-2">
                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                <div class="flex-grow-1 subheading">German Aerospace Center, Institute of Robotics and Mechatronics</div>
                                <div class="flex-shrink-0"><span class="text-primary">September 2019 - Present</span></div>
                            </div>
                            <ul class="fa-ul mb-0">
                            <li>
                                <span class="fa-li"><i class="fa fa-angle-right"></i></span>
                                Master’s thesis topic: Probabilistic Graphical Model for RGB-D object recognition
                            </li>
                            <li>
                                <span class="fa-li"><i class="fa fa-angle-right"></i></span>
                                Development of an classifier ensemble for RGB-D object recognition
                            </li>
                            <li>
                                <span class="fa-li"><i class="fa fa-angle-right"></i></span>
                                Generation of a dataset of household objects for classification
                            </li>
                            </ul>
                        </div>
                        <div class="mb-2">
                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                <div class="flex-grow-1 subheading">Chair for Data Science, Technical University of Munich</div>
                                <div class="flex-shrink-0"><span class="text-primary">April 2014 - August 2014</span></div>
                            </div>
                            <ul class="fa-ul mb-0">
                            <li>
                                <span class="fa-li"><i class="fa fa-angle-right"></i></span>
                                Development of a actor-critic reinforcement learning algorithm for laser welding
                            </li>
                            <li>
                                <span class="fa-li"><i class="fa fa-angle-right"></i></span>
                                Implementation of this algorithm in C
                            </li>
                            </ul>
                            </div>
                        <div class="mb-2">
                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                <div class="flex-grow-1 subheading">Infineon Technologies AG, Product development RF & Protection Devices</div>
                                <div class="flex-shrink-0"><span class="text-primary">December 2013 - March 2014</span></div>
                            </div>
                            <ul class="fa-ul mb-0">
                            <li>
                                <span class="fa-li"><i class="fa fa-angle-right"></i></span>
                                Assembly of test setups for high frequency sensors
                            </li>
                            <li>
                                <span class="fa-li"><i class="fa fa-angle-right"></i></span>
                                Execution of test series
                            </li>
                            </ul>
                        </div>
                        <div class="mb-2">
                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                <div class="flex-grow-1 subheading">TUM Create, Prototyping & Testbedding, Singapore</div>
                                <div class="flex-shrink-0"><span class="text-primary">August 2013 - Ocotober 2013</span></div>
                            </div>
                            <ul class="fa-ul mb-0">
                            <li>
                                <span class="fa-li"><i class="fa fa-angle-right"></i></span>
                                Implementation of a controller for a personalised air condition system of a fully electrical vehicle in MATLAB Simulink
                            </li>
                            <li>
                                <span class="fa-li"><i class="fa fa-angle-right"></i></span>
                                Assistance in prototype assembly of the vehicle
                            </li>
                            </ul>
                        </div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Education-->
            <section class="resume-section" id="education">
                <div class="resume-section-content">
                    <h2 class="mb-5">Education</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">PhD: <span class="heading text-secondary"> Computer Science </span> </h3>
                            <!--<div class="subheading mb-3">Technical University of Munich</div>-->
                            <div class="subheading">Chair of Computer Vision & Artificial Intelligence, Technical University of Munich</div>
                            <!--<p>GPA: 3.23</p>-->
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Ongoing</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Master of Science: <span class="heading text-secondary"> Electrical Engineering and Information Technology </span> </h3>
                            <!--<div class="subheading mb-3">Technical University of Munich</div>-->
                            <div class="subheading">Technical University of Munich</div>
                            <!--<p>GPA: 3.23</p>-->
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">March 2016</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Bachelor of Science: <span class="heading text-secondary"> Electrical Engineering and Information Technology </span> </h3>
                            <div class="subheading">Technical University of Munich</div>
                            <!--<div> </div>-->
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">September 2013</span></div>
                    </div>
                    <div>
                        <h3 class="mb-1">Exchange Semester</h3>
                        <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                            <div class="flex-grow-1 subheading">Politecnico di Torino, Italy</div>
                            <div class="flex-shrink-0"><span class="text-primary">October 2011 ‒ June 2012</span></div>
                        </div>
                        <div class="d-flex flex-column flex-md-row justify-content-between">
                            <div class="flex-grow-1 subheading">Universidad Nacional de Colombia, Bogotá</div>
                            <div class="flex-shrink-0"><span class="text-primary">August 2014 ‒ December 2014</span></div>
                        </div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Skills-->
            <section class="resume-section" id="skills">                
                <div class="resume-section-content">
                    <h2 class="mb-5">Skills</h2>
                    <div class="container">
                        <div class="row justify-content-between">
                            <div class="col-4" align="center">
                                <div class="featurette-icon"><i class="fab fa-python fa-5x"></i></div>
                                <h4>python</h4>
                                <p>90%</p>
                            </div>
                            <div class="col-4" align="center">
                                <div class="featurette-icon"><i class="fas fa-robot fa-5x"></i></div>
                                <h4>ROS</h4>
                                <p>75%</p>
                            </div>
                            <div class="col-4" align="center">
                                <div class="featurette-icon"><i class="fas fa-copyright fa-5x"></i></div>
                                <h4>C++</h4>
                                <p>60%</p>
                            </div>
                        </div>
                        <div class="row justify-content-between">
                            <div class="col-4" align="center">
                                <div class="featurette-icon"><i class="fab fa-linux fa-5x"></i></div>
                                <h3>Linux</h4>
                                <p>70%</p>
                            </div>
                            <div class="col-4" align="center">
                                <div class="featurette-icon"><i class="fab fa-apple fa-5x"></i></div>
                                <h4>MacOS</h4>
                                <p>80%</p>
                            </div>
                            <div class="col-4" align="center">
                                <div class="featurette-icon"><i class="fab fa-git fa-5x"></i></div>
                                <h4>git</h4>
                                <p>80%</p>
                            </div>
                        </div>
                        <div class="row justify-content-between">
                            <div class="col-4" align="center">
                                <div class="featurette-icon"><i class="fas fa-code fa-5x"></i></div>
                                <h4>Pytorch</h4>
                                <p>80%</p>
                            </div>
                            <div class="col-4" align="center">
                                <div class="featurette-icon"><i class="fas fa-code fa-5x"></i></div>
                                <h4>Tensorflow</h4>
                                <p>70%</p>
                            </div>
                            <div class="col-4" align="center">
                                <div class="featurette-icon"><i class="fas fa-code fa-5x"></i></div>
                                <h4>Latex</h4>
                                <p>80%</p>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Projects -->
            <section class="resume-section" id="projects">
                <div class="resume-section-content">
                    <h2 class="mb-5">Projects</h2>
                    <div class="card-columns">
                        <div class="card">
                            <a href="https://www.dlr.de/rm/desktopdefault.aspx/tabid-11409/#gallery/29194" class="card-image hover-overlay">
                                <img src="projects/industrial_mobile_manipulation/fof_1.png" class="card-img-top" alt="...">
                            </a>
                            <div class="card-body">
                                <!--<h5 class="card-title"> <a href="https://www.dlr.de/rm/desktopdefault.aspx/tabid-11409/#gallery/29194"> Industrial Mobile Manipulation </a> </h5>-->
                                <h5 class="card-title"> Industrial Mobile Manipulation </h5>
                                <p class="card-text">Industrial, Autonomous Mobile Manipulation</p>
                                <p class="card-text"><small class="text-muted">Project ongoing</small></p>
                            </div>
                        </div>
                        <div class="card">
                            <div class="card-body">
                                <a href="http://www.robdream.eu/index.html%3Fq=home.html" class="card-image hover-overlay">
                                    <img src="projects/robdream/logo.png" class="card-img-top" alt="" class="img-responsive">
                                </a>
                                <p class="card-text"><small class="text-muted">Feburary 2015 - May 2017</small></p>
                            </div>
                        </div>
                        <!--
                        <div class="card p-3">
                          <blockquote class="blockquote mb-0 card-body">
                            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante.</p>
                            <footer class="blockquote-footer">
                              <small class="text-muted">
                                Someone famous in <cite title="Source Title">Source Title</cite>
                              </small>
                            </footer>
                          </blockquote>
                        </div>
                        -->
                        <div class="card">
                            <!--<img src="..." class="card-img-top" alt="...">-->
                            <div class="card-body">
                                <h3 class="card-title"><a href="https://cordis.europa.eu/project/id/951992"> VeriDream </a></h3>
                                <p class="card-text"><small class="text-muted">November 2020 ongoing</small></p>
                            </div>
                        </div>
                        <div class="card">
                            <!--<img src="..." class="card-img-top" alt="...">-->
                            <div class="card-body">
                                <h3 class="card-title"><a href="https://factory-of-the-future.dlr.de"> Factory of the Future </a></h3>
                                <p class="card-text"><small class="text-muted">Ongoing project</small></p>
                            </div>
                        </div>
                        <!--
                            <div class="card bg-primary text-white text-center p-3">
                              <blockquote class="blockquote mb-0">
                                <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat.</p>
                                <footer class="blockquote-footer text-white">
                                  <small>
                                    Someone famous in <cite title="Source Title">Source Title</cite>
                                  </small>
                                </footer>
                              </blockquote>
                            </div>
                        -->
                        <div class="card">
                            <a href="https://www.dlr.de/rm/desktopdefault.aspx/tabid-11852/#gallery/35446" class="card-image hover-overlay">
                                <img src="projects/arches/lru.jpg" class="card-img-top" alt="...">
                            </a>
                            <div class="card-body">
                                <!--<h5 class="card-title"> <a href="https://www.dlr.de/rm/desktopdefault.aspx/tabid-11409/#gallery/29194"> Industrial Mobile Manipulation </a> </h5>-->
                                <h5 class="card-title"> Arches </h5>
                                <!--<p class="card-text">Industrial, Autonomous Mobile Manipulation</p>-->
                                <p class="card-text"><small class="text-muted">Project ongoing</small></p>
                            </div>
                        </div>
                        <!--
                            <div class="card text-center">
                              <div class="card-body">
                                <h5 class="card-title">Card title</h5>
                                <p class="card-text">This card has a regular title and short paragraphy of text below it.</p>
                                <p class="card-text"><small class="text-muted">Last updated 3 mins ago</small></p>
                              </div>
                            </div>
                            -->
                        <!--
                            <div class="card p-3 text-right">
                              <blockquote class="blockquote mb-0">
                                <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante.</p>
                                <footer class="blockquote-footer">
                                  <small class="text-muted">
                                    Someone famous in <cite title="Source Title">Source Title</cite>
                                  </small>
                                </footer>
                              </blockquote>
                            </div>
                            -->
                        <!--
                            <div class="card">
                              <div class="card-body">
                                <h5 class="card-title">Card title</h5>
                                <p class="card-text">This is another card with title and supporting text below. This card has some additional content to make it slightly taller overall.</p>
                                <p class="card-text"><small class="text-muted">Last updated 3 mins ago</small></p>
                              </div>
                            </div>
                          </div>
                          -->
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Publications -->
            <section class="resume-section" id="publications">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publications</h2>
                    <div class="panel-group" id="accordion" role="tablist" aria-multiselectable="true">
                        <!-- TEMPLATE
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading##">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            #######NAME######
                                            <a role="button" data-toggle="collapse" data-parent="#accordion" href="#collapse##" aria-expanded="false" aria-controls="collapse##">
                                            ######TITLE#######
                                            </a>
                                            <div id="collapse##" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="heading##">
                                                <div class="card card-body">
                                                    <p class="font-weight-bold mb-2"> Abstract: </p> </br>
                                                    <p class="mb-3">
                                                    abstract here
                                                    </p>
                                                    <div class="font-italic">
                                                    Proceedings 
                                                    </div>
                                                </div>
                                            </div>
                                            ##########Conference
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2020</span></div>
                                </div>
                            </div>
                        </div>
                        -->
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading##">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            M. Durner*, W. Boerdijk*, M. Sundermeyer, W. Friedl, Z.-C. Márton, and R. Triebel.
                                            <a role="button" data-toggle="collapse" data-parent="#accordion" href="#collapse##" aria-expanded="false" aria-controls="collapse##">
                                            Unknown Object Segmentation from Stereo Images
                                            </a>
                                            <div class="modal fade" id="abstractModal19" tabindex="-1" role="dialog" aria-labelledby="abstractModal19Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal19Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                  Although instance-aware perception is a key prerequisite for many autonomous robotic applications, most of the methods only partially solve the problem by focusing solely on known object categories. However, for robots interacting in dynamic and cluttered environments, this is not realistic and severely limits the range of potential applications. Therefore, we propose a novel object instance segmentation approach that does not require any semantic or geometric information of the objects beforehand. In contrast to existing works, we do not explicitly use depth data as input, but rely on the insight that slight viewpoint changes, which for example are provided by stereo image pairs, are often sufficient to determine object boundaries and thus to segment objects. Focusing on the versatility of stereo sensors, we employ a transformer-based architecture that maps directly from the pair of input images to the object instances. This has the major advantage that instead of a noisy, and potentially incomplete depth map as an input, on which the segmentation is computed, we use the original image pair to infer the object instances and a dense depth map. In experiments in several different application domains, we show that our Instance Stereo Transformer (INSTR) algorithm outperforms current state-of-the-art methods that are based on depth maps. Training code and pretrained models will be made available.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      arXiv preprint arXiv:2011.03279
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            arXiv
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://arxiv.org/pdf/2103.06796.pdf" target="_blank" rel="noopener">
                                            pdf
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2021</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading19">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            W. Boerdijk, M. Sundermeyer, M. Durner, and R. Triebel.
                                            <a href="" data-toggle="modal" data-target="#abstractModal19">
                                                "What's This?"--Learning to Segment Unknown Objects from Manipulation Sequences.
                                            </a>
                                            <div class="modal fade" id="abstractModal19" tabindex="-1" role="dialog" aria-labelledby="abstractModal19Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal19Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                  We present a novel framework for self-supervised grasped object segmentation with a robotic manipulator. Our method successively learns an agnostic foreground segmentation followed by a distinction between manipulator and object solely by observing the motion between consecutive RGB frames. In contrast to previous approaches, we propose a single, end-to-end trainable architecture which jointly incorporates motion cues and semantic knowledge. Furthermore, while the motion of the manipulator and the object are substantial cues for our algorithm, we present means to robustly deal with distraction objects moving in the background, as well as with completely static scenes. Our method neither depends on any visual registration of a kinematic robot or 3D object models, nor on precise hand-eye calibration or any additional sensor data. By extensive experimental evaluation we demonstrate the superiority of our framework and provide detailed insights on its capability of dealing with the aforementioned extreme cases of motion. We also show that training a semantic segmentation network with the automatically labeled data achieves results on par with manually annotated training data. Code and pretrained models will be made publicly available.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      arXiv preprint arXiv:2011.03279
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            arXiv
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://arxiv.org/pdf/2011.03279.pdf" target="_blank" rel="noopener">
                                            pdf
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2020</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading18">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            M. Schuster, M. Müller, S. Brunner, H. Lehner, P. Lehner, R. Sakagami, A. Dömel, L. Meyer, B. Vodermayer, R. Giubilato, M. Vayugundla, J. Reill, F. Steidle, I. v. Bargen, K. Bussmann, R. Belder, P. Lutz, W. Stürzl, M. Smíšek, M. Maier, S. Stoneman, A. F. Prince, B. Rebele, M. Durner, E. Staudinger, S. Zhang, R. Pöhlmann, E. Bischoff, C. Braun, S. Schröder, E. Dietz, S. Frohmann, A. Börner, H.-W. Hübers, B. Foing, R. Triebel, A. Albu-Schäffer, and A. Wedler.
                                            <a href="" data-toggle="modal" data-target="#abstractModal18">
                                                The ARCHES space-analogue demonstration mission: towards heterogeneous teams of autonomous robots for collaborative scientific sampling in planetary exploration.
                                            </a>
                                            <div class="modal fade" id="abstractModal18" tabindex="-1" role="dialog" aria-labelledby="abstractModal18Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModalLabel18">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                      Teams of mobile robots will play a crucial role in future missions to explore the surfaces of extraterrestrial bodies. Setting up infrastructure and taking scientific samples are expensive tasks when operating in distant, challenging, and unknown environments. In contrast to current single-robot space missions, future heterogeneous robotic teams will increase efficiency via enhanced autonomy and parallelization, improve robustness via functional redundancy, as well as benefit from complementary capabilities of the individual robots. In this letter, we present our heterogeneous robotic team, consisting of flying and driving robots that we plan to deploy on scientific sampling demonstration missions at a Moon-analogue site on Mt. Etna, Sicily, Italy in 2021 as part of the ARCHES project. We describe the robots' individual capabilities and their roles in two mission scenarios. We then present components and experiments on important tasks therein: automated task planning, high-level mission control, spectral rock analysis, radio-based localization, collaborative multi-robot 6D SLAM in Moon-analogue and Mars-like scenarios, and demonstrations of autonomous sample return.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      IEEE Robotics and Automation Letters (RA-L), Volume: 5, Issue: 4, pages 5315-5322, October 2020
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            RA-L
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://elib.dlr.de/136354/1/2020_RAL_IROS_multirobot_sampling_v1.1_elib.pdf" target="_blank" rel="noopener">
                                            pdf
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2020</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading17">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            I. Rodríguez, K. Nottensteiner, D. Leidner, M. Durner, F. Stulp, and A. Albu-Schäffer
                                            <a href="" data-toggle="modal" data-target="#abstractModal17">
                                                Pattern Recognition for Knowledge Transfer in Robotic Assembly Sequence Planning.
                                            </a>
                                            <div class="modal fade" id="abstractModal17" tabindex="-1" role="dialog" aria-labelledby="abstractModal17Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal17Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                      The autonomous assembly of customized products is highly demanded in future manufacturing scenarios. This requires robotic systems being able to adapt to individual products without increasing overall production time. However, increasingly complex assemblies lead to a growing number of potential assembly sequences that have to be considered. To cope with this, we present an algorithm that is able to transfer previously identified assembly constraints to novel product variants. This reduces the search space, and thus planning times. The approach consist of three main steps. 1) Deduct semantic assembly constraints, from an analysis of feasible and unfeasible solutions. 2) Match key features of assemblies on a semantic level, by performing graph matching in the representation of the assemblies. 3) Use pattern recognition and classification based on machine learning techniques to transfer the knowledge of constraints for sub-assemblies into the complete assembly. We demonstrate our contributions on a two-armed robotic setup that assembles product variants out of aluminum profiles.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      IEEE Robotics and Automation Letters (RA-L), Volume: 5, Issue: 2, pages: 3666-3673, April 2020
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            RA-L
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://elib.dlr.de/135368/1/20_ICRA_PatternRecognitionForKnowledgeTransfer_RAL_version.pdf" target="_blank" rel="noopener">
                                            pdf
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2020</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading16">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            M. Sundermeyer, Z.-C. Márton, M. Durner, and R. Triebel.
                                            <a href="" data-toggle="modal" data-target="#abstractModal16">
                                                Augmented autoencoders: Implicit 3D orientation learning for 6D object detection.
                                            </a>
                                            <div class="modal fade" id="abstractModal16" tabindex="-1" role="dialog" aria-labelledby="abstractModal16Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModalLabel16">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                      We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization. This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries. Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space. Our pipeline achieves state-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D domain. We also evaluate on the LineMOD dataset where we can compete with other synthetically trained approaches. We further increase performance by correcting 3D orientation estimates to account for perspective errors when the object deviates from the image center and show extended results. Our code is available here https://github.com/DLR-RM/AugmentedAutoencoder.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      International Journal of Computer Vision (IJCV), 2020
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            IJCV
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://link.springer.com/article/10.1007/s11263-019-01243-8" target="_blank" rel="noopener">
                                            pdf 
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2020</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading15">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            W. Boerdijk, M. Sundermeyer, M. Durner, and R. Triebel.
                                            <a href="" data-toggle="modal" data-target="#abstractModal15">
                                                Self-Supervised Object-in-Gripper Segmentation from Robotic Motions.
                                            </a>
                                            <div class="modal fade" id="abstractModal15" tabindex="-1" role="dialog" aria-labelledby="abstractModal15Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal15Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                      We present a novel technique to automatically generate annotated data for important robotic perception tasks such as object segmentation and 3D object reconstruction using a robot manipulator. Our self-supervised method can segment unknown objects from a robotic gripper in RGB video sequences by exploiting motion and temporal cues. The key aspect of our approach in contrast to existing systems is its independence of any hardware specifics such as extrinsic and intrinsic camera calibration and a robot model. We achieve this using a two-step process: First, we learn to predict segmentation masks for our given manipulator using optical flow estimation. Then, these masks are used in combination with motion cues to automatically distinguish between the manipulator, the background, and the unknown, grasped object. We perform a thorough comparison with alternative baselines and approaches in the literature. The obtained object views and masks are suitable training data for segmentation networks that generalize to novel environments and also allow for watertight 3D object reconstruction.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      Conference on Robot Learning (CORL), virtual, 2020
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            CORL
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://elib.dlr.de/139332/1/wout_boerdijk20.pdf" target="_blank" rel="noopener">
                                            pdf
                                            </a>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://www.youtube.com/watch?v=srEwuuIIgzI" target="_blank" rel="noopener">
                                            video
                                            </a>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://www.youtube.com/watch?v=LPsLYMinWKU" target="_blank" rel="noopener">
                                            talk
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2020</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading14">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            M. Schuster, B. Rebele, M. Müller, S. Brunner, A. Dömel, B. Vodermayer, R. Giubilato, M. Vayugundla, H. Lehner, P. Lehner, F. Steidle, L. Meyer, K. Bussmann, J. Reill, W. Stürzl, I. v. Bargen, R. Sakagami, M. Smisek, M. Durner, E. Staudinger, R. Pöhlmann, S. Zhang, C. Braun, E. Dietz, S. Frohmann, S. Schröder, A. Börner, H.-W. Hübers, R. Triebel, B. Foing, A. Albu-Schäffer, and A. Wedler.
                                            <a href="" data-toggle="modal" data-target="#abstractModal14">
                                                The ARCHES moon-analogue demonstration mission: Towards teams of autonomous robots for collaborative scientific sampling in lunar environments.
                                            </a>
                                            <div class="modal fade" id="abstractModal14" tabindex="-1" role="dialog" aria-labelledby="abstractModal14Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal14Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                      Teams of mobile robots will play a crucial role in future missions to explore the surface of the Moon. Setting up infrastructure and taking scientific samples are expensive tasks when operating in such distant, challenging, and unknown environments. In contrast to current single-robot space missions, future heterogeneous robotic teams will increase efficiency via enhanced autonomy and parallelization, improve robustness via functional redundancy, as well as benefit from complementary capabilities of the individual robots. We present our heterogeneous robotic team, consisting of flying and driving robots that we plan to deploy on scientific sampling demonstration missions at a Moon-analogue site on Mt. Etna, Sicily, Italy in 2021 as part of the ARCHES project. We give a brief description of the robots' complementary capabilities and present their roles in two mission scenarios.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      European Lunar Symposium (ELS), Padua, Italy(virtual), 2020
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            ELS
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2020</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading13">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            M. Sundermeyer, M. Durner, E.-Y. Puang, Z.-C. Márton, N. Vaskevicius, K. Arras, and R. Triebel.
                                            <a href="" data-toggle="modal" data-target="#abstractModal13">
                                                Multi-path learning for object pose estimation across domains.
                                            </a>
                                            <div class="modal fade" id="abstractModal13" tabindex="-1" role="dialog" aria-labelledby="abstractModal13Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal13Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                  We introduce a scalable approach for object pose estimation trained on simulated RGB views of multiple 3D models together. We learn an encoding of object views that does not only describe an implicit orientation of all objects seen during training, but can also relate views of untrained objects. Our single-encoder-multi-decoder network is trained using a technique we denote" multi-path learning": While the encoder is shared by all objects, each decoder only reconstructs views of a single object. Consequently, views of different instances do not have to be separated in the latent space and can share common features. The resulting encoder generalizes well from synthetic to real data and across various instances, categories, model types and datasets. We systematically investigate the learned encodings, their generalization, and iterative refinement strategies on the ModelNet40 and T-LESS dataset. Despite training jointly on multiple objects, our 6D Object Detection pipeline achieves state-of-the-art results on T-LESS at much lower runtimes than competing approaches.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), virtual, 2020
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            CVPR
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Sundermeyer_Multi-Path_Learning_for_Object_Pose_Estimation_Across_Domains_CVPR_2020_paper.pdf" target="_blank" rel="noopener">
                                            pdf
                                            </a>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://www.youtube.com/watch?v=pZi1xuouSR8" target="_blank" rel="noopener">
                                            video
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2020</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading12">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            E.-Y. Puang, P. Lehner, Z.-C. Márton, M. Durner, R. Triebel, and A. Albu-Schäffer.
                                            <a href="" data-toggle="modal" data-target="#abstractModal12">
                                                Visual Repetition Sampling for Robot Manipulation Planning
                                            </a>
                                            <div class="modal fade" id="abstractModal12" tabindex="-1" role="dialog" aria-labelledby="abstractModal12Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal12Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                  One of the main challenges in sampling-based motion planners is to find an efficient sampling strategy. While methods such as Rapidly-exploring Random Tree (RRT) have shown to be more reliable in complex environments than optimization-based methods, they often require longer planning times, which reduces their usability for real-time applications. Recently, biased sampling methods have shown to remedy this issue. For example Gaussian Mixture Models (GMMs) have been used to sample more efficiently in feasible regions of the configuration space. Once the GMM is learned, however, this approach does not adapt its biases to individual planning scene during inference. Hence, we propose in this work a more efficient sampling strategy to further bias the GMM based on visual input upon query. We employ an autoencoder trained entirely in simulation to extract features from depth images and use the latent representation to adjust the weights of each mixture components in the GMM. We show empirically that this improves the sampling efficiency of an RRT motion planner in both real and simulated scenes.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      Proceedings of the IEEE/RSJ International Conference on Robotics and Automation (ICRA), Montreal, Canada, May 2019
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            ICRA
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://elib.dlr.de/128182/1/ICRA19_0465_FI.pdf" target="_blank" rel="noopener">
                                            pdf
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2019</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading11">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            J. Feng*, M. Durner*, Z.-C. Márton, F. Balint-Benczedi, and R. Triebel.
                                            <a href="" data-toggle="modal" data-target="#abstractModal11">
                                                Introspective Robot Perception using Smoothed Predictions from Bayesian Neural Networks.
                                            </a>
                                            <div class="modal fade" id="abstractModal11" tabindex="-1" role="dialog" aria-labelledby="abstractModal11Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal11Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                      This work focuses on improving uncertainty estimation in the field of object classification from RGB images and demonstrates its benefits in two robotic applications. We employ a Bayesian Neural Network (BNN), and evaluate two practical inference techniques to obtain better uncertainty estimates, namely Concrete Dropout (CDP) and Kronecker-factored Laplace Approximation (LAP). We show a performance increase using more reliable uncertainty estimates as unary potentials within a Conditional Random Field (CRF), which is able to incorporate contextual information as well. Furthermore, the obtained uncertainties are exploited to achieve domain adaptation in a semi-supervised manner, which requires less manual efforts of annotating data. We evaluate our approach on two public benchmark datasets that are relevant for robot perception tasks.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      International Symposium on Robotics Research (ISRR), Hanoi, Vietnam, October 2019
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            ISRR
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://elib.dlr.de/133562/1/uncertainty_improvement.pdf" target="_blank" rel="noopener">
                                            pdf
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2019</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading10">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            M. Brucker, M. Durner, Z.-C. Marton, F. Balint-Benczedi, M. Sundermeyer, and R. Triebel.
                                            <a href="" data-toggle="modal" data-target="#abstractModal10">
                                                6DoF Pose Estimation for Industrial Manipulation based on Synthetic Data.
                                            </a>
                                            <div class="modal fade" id="abstractModal10" tabindex="-1" role="dialog" aria-labelledby="abstractModal10Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="exampleModal10Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                      We present a perception system for mobile manipulation tasks. The primary design goal of the proposed system is to minimize human interaction during system setup which is achieved by several means, such as automatic training data generation, the use of simulated training data, and 3D model based geometric matching. We employ a state-of-the art deep-learning based bounding box detector for rough localization of objects and a Point Pair Feature based matching algorithm for 6DoF pose estimation. The proposed approach shows promising results on our recently published dataset for industrial object detection and pose estimation. Furthermore, the system’s performance during four days of live operation at the Automatica 2018 trade fair is analyzed and failure cases are presented and discussed.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      International Symposium on Experimental Robotics (ISER), Buenos Aires, Argentina, November 2018
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            ISER
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2018</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading9">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            C. Nissler, M. Durner, Z.-C. Marton, and R. Triebel.
                                            <a href="" data-toggle="modal" data-target="#abstractModal9">
                                                Simultaneous Calibration and Mapping.
                                            </a>
                                            <div class="modal fade" id="abstractModal9" tabindex="-1" role="dialog" aria-labelledby="abstractModal9Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal9Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                      We present evaluation experiments of a hand-eye calibration and camera-camera calibration method, which is applicable to cases where classical calibration methods fail. As described in our earlier works, the calibration works by performing rotational movements with the robot and estimating the rotational axes by tracking fiducial markers or other static parts of the environment (if the camera is moved with the robot, as in this experiment; otherwise tracking the robot itself or markers on it with a static camera). We extend our earlier work by virtually increasing the field of view of the cameras by using a mapping approach. We compare our results with an extended classical approach for the challenging case of calibrating a compliant humanoid robot having cameras with non-overlapping fields of view.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      International Symposium on Experimental Robotics (ISER), Buenos Aires, Argentina, November 2018
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            ISER
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://elib.dlr.de/124822/1/learnAprilTrafos.pdf" target="_blank" rel="noopener">
                                            pdf
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2018</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading8">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            M. Sundermeyer, Z.-C. Márton, M. Durner, M. Brucker, and R. Triebel.
                                            <a href="" data-toggle="modal" data-target="#abstractModal8">
                                                Implicit 3D Orientation Learning for 6D Object Detection from RGB Images.
                                            </a>
                                            <div class="modal fade" id="abstractModal8" tabindex="-1" role="dialog" aria-labelledby="abstractModal8Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal8Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                  We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization. This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries. Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space. Experiments on the T-LESS and LineMOD datasets show that our method outperforms similar model-based approaches and competes with state-of-the art approaches that require real pose-annotated images.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, September 2018
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            ECCV (<span class="font-weight-bold">Best Paper Award</span>)
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Martin_Sundermeyer_Implicit_3D_Orientation_ECCV_2018_paper.pdf" target="_blank" rel="noopener">
                                            pdf
                                            </a>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://github.com/DLR-RM/AugmentedAutoencoder" target="_blank" rel="noopener">
                                            code
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2018</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading7">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            M. Sundermeyer, E. Y. Puang, Z.-C. Márton, M. Durner, and R. Triebel.
                                            <a href="" data-toggle="modal" data-target="#abstractModal7">
                                                Learning Implicit Representations of 3D Object Orientations from RGB.
                                            </a>
                                            <div class="modal fade" id="abstractModal7" tabindex="-1" role="dialog" aria-labelledby="abstractModal7Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal7Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                      This work presents a fast and robust algorithm for object orientation estimation that is solely trained on synthetic views rendered from a 3D model. We introduce a dense encoder- decoder architecture that learns implicit representations of 3D object orientations. Since our training is self-supervised, we avoid the necessity of real, pose-annotated training data. Furthermore, it prevents issues related to ambiguous object views. To encode latent representations that are robust against occlusions, clutter and the differences between synthetic and real data, a new domain randomization strategy is proposed. We motivate our approach by experiments on abstract 2D shapes and evaluate it on the challenging T-LESS dataset. In addition to the results in this paper, we provide a live presentation of the system during the workshop, on a Nvidia Jetson TX2 board.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      ICRA Workshop: Representing a Complex World: Perception, Inference, and Learning for Joint Semantic, Geometric, and Physical Understanding, Brisbane, Australia, May 2018
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            ICRA WS (<span class="font-weight-bold">Best Demo Award</span>)
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://elib.dlr.de/125553/1/ICRA-MRP18_paper_17.pdf" target="_blank" rel="noopener">
                                            pdf
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2018</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading6">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            M. Brucker*, M. Durner*, R. Ambruş*, Z.-C. Márton, A. Wendt, P. Jensfelt, K. Arras, and R. Triebel.
                                            <a href="" data-toggle="modal" data-target="#abstractModal6">
                                                Semantic Labeling of Indoor Environments from 3D RGB Maps.
                                            </a>
                                            <div class="modal fade" id="abstractModal6" tabindex="-1" role="dialog" aria-labelledby="abstractModal6Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal6Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                      We present an approach to automatically assign semantic labels to rooms reconstructed from 3D RGB maps of apartments. Evidence for the room types is generated using state-of-the-art deep-learning techniques for scene classification and object detection based on automatically generated virtual RGB views, as well as from a geometric analysis of the map's 3D structure. The evidence is merged in a conditional random field, using statistics mined from different datasets of indoor environments. We evaluate our approach qualitatively and quantitatively and compare it to related methods.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      Proceedings of the IEEE/RSJ International Conference on Robotics and Automation (ICRA), Brisbane, Australia, May 2018
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            ICRA
                                        </div>
                                        <div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://www.dlr.de/rm/desktopdefault.aspx/tabid-4817/21153_read-49339/" target="_blank" rel="noopener">
                                            dataset
                                            </a>
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2017</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading5">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            M. Ullrich, H. Ali, M. Durner, Z.-C. Marton, and R. Triebel.
                                            <a href="" data-toggle="modal" data-target="#abstractModal5">
                                                Selecting CNN features for online learning of 3D objects.
                                            </a>
                                            <div class="modal fade" id="abstractModal5" tabindex="-1" role="dialog" aria-labelledby="abstractModalLabel5" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal5Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                      We present a novel method for classifying 3D objects that is particularly tailored for the requirements in robotic applications. The major challenges here are the comparably small amount of available training data and the fact that often data is perceived in streams and not in fixed-size pools. Traditional state-of-the-art learning methods, however, require a large amount of training data, and their online learning capabilities are usually limited. Therefore, we propose a modality-specific selection of convolutional neural networks (CNN), pre-trained or fine-tuned, in combination with a classifier that is designed particularly for online learning from data streams, namely the Mondrian Forest (MF). We show that this combination of trained features obtained from a CNN can be improved further if a feature selection algorithm is applied. In our experiments, we use the resulting features both with a MF and a linear Support Vector Machine (SVM). With SVM we beat the state of the art on an RGB-D dataset, while with MF a strong result for active learning is achieved.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Vancouver, Canada, September 2017
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            IROS
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2017</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading2">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            M. Durner, Z.-C. Marton, S. Kriegel, M. Brucker, S. Riedel, D. Meinzer, and R. Triebel. 
                                            <a href="" data-toggle="modal" data-target="#abstractModal4">
                                                Automated Benchmarks and Optimization of Perception Tasks.
                                            </a>
                                            <div class="modal fade" id="abstractModal4" tabindex="-1" role="dialog" aria-labelledby="abstractModal4Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal4Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                  Advanced robots operating in complex and dynamic environments require intelligent perception algorithms to navigate collision-free, analyze scenes, recognize relevant objects, and manipulate them. Nowadays, the perception of mobile manipulation systems often fails if the context changes due to a variation e.g. in the lightning conditions, the utilized objects, the manipulation area, or the environment. Then, a robotic expert is needed who needs to adjust the parameters of the perception algorithm and the utilized sensor or even select a better method or sensor.
 Thus, a high-level cognitive ability that is required for operating alongside humans is to continuously improving their performance based on introspection. This adaptability to changing situations requires different aspects of machine learning, e.g. storing experiences for life-long learning,
 creating annotated datasets for supervised learning through user interaction, Bayesian optimization to avoid brute-force search in high-dimensional data, and a unified representation of data and meta-data to facilitate knowledge transfer. Here we present how we automated and integrated different aspects of these.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      IROS 2nd Workshop on Machine Learning Methods for High-Level Cognitive Capabilities in Robotics, Vancouver, Canada, September 2017
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            IROS WS
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2017</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading3">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            M. Durner*, S. Kriegel*, S. Riedel, M. Brucker, Z.-C. Marton, F. Balint-Benczedi, and R. Triebel.
                                            <a href="" data-toggle="modal" data-target="#abstractModal3">
                                                Experience-based optimization of robotic perception.
                                            </a>
                                            <div class="modal fade" id="abstractModal3" tabindex="-1" role="dialog" aria-labelledby="abstractModalLabel3" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModalLabel3">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                  As the performance of key perception tasks heavily depends on their parametrization, deploying versatile robots to different application domains will also require a way to tune these changing scenarios by their operators. As many of these tunings are found by trial and error basically by experts as well, and the quality criteria change from application to application, we propose a Pipeline Optimization Framework that helps overcoming lengthy setup times by largely automating this process. When deployed, fine-tuning optimizations as presented in this paper can be initiated on pre-recorded data, dry runs, or automatically during operation. Here, we quantified the performance gains for two crucial modules based on ground truth annotated data. We release our challenging THR dataset, including evaluation scenes for two application scenarios.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      18th International Conference on Advanced Robotics (ICAR), HongKong, July 2017
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            ICAR (<span class="font-weight-bold">Finalist for Best Paper Award</span>)
                                        </div>
                                            <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://www.dlr.de/rm/desktopdefault.aspx/tabid-11999/" target="_blank" rel="noopener">
                                            dataset
                                            </a>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2017</span></div>
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading2">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                            F. Balint-Benczedi, Z.-C. Marton, M. Durner, and M. Beetz.
                                            <a href="" data-toggle="modal" data-target="#abstractModal2">
                                                Storing and Retrieving Perceptual Episodic Memories for Long-term Manipulation Tasks.
                                            </a>
                                            <div class="modal fade" id="abstractModal2" tabindex="-1" role="dialog" aria-labelledby="abstractModal2Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal2Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                      With recent technological advances, robotic agents are increasingly capable of performing ever more sophisticated manipulation tasks. Perceptual capabilities of these robots need to be able to adapt to the wide variety of tasks they are to perform. Remembering what a robot has seen, what the rationale was behind the decisions it took or how it ended up understanding the world as it did, are important questions if we want perception capabilities that can scale towards real-world manipulation. We present a robotic perception system that generates perceptual episodic memories during the execution of a task. To allow easy retrieval of these memories we introduce an object and scene description language that serves as a layer of abstraction between the structure of the perception logs and the semantic interpretation of these. The description language can be used through a query interface to retrieve specific parts of the generated episodic memory. The purpose of the proposed system is two-fold: to enable on-line retrospection and specialized training of perception routines and to enable researchers to interactively explore perception results.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                      18th International Conference on Advanced Robotics (ICAR), HongKong, July 2017
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            ICAR (<span class="font-weight-bold">Finalist for Best Paper Award</span>)
                                        </div>
                                    </div>
                                    <div class="flex-shrink-0"><span class="text-primary">2017</span></div>
                                </div>
                            </div>
                        </div>

                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="heading1">
                                <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                                    <div class="col-11">
                                    <!--<div class="flex-grow-1">-->
                                        <div class=="d-inline-block"> 
                                            <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                                                M. Durner, Z.-C. Marton, U. Hillenbrand, H. Ali, and M. Kleinsteuber.                                     
                                             <!--<a href="#" onclick="return false;" role="button" class="btn-link" data-toggle="collapse" data-target="#collapseExample" aria-expanded="false" aria-controls="collapseExample">   -->
                                            <a href="" data-toggle="modal" data-target="#abstractModal0">
                                                Active classifier selection for RGB-D object categorization using a Markov Random Field ensemble method.
                                            </a>
                                            <div class="modal fade" id="abstractModal0" tabindex="-1" role="dialog" aria-labelledby="abstractModal0Label" aria-hidden="true">
                                              <div class="modal-dialog modal-lg" role="document">
                                                <div class="modal-content">
                                                  <div class="modal-header">
                                                    <h5 class="modal-title" id="abstractModal0Label">Abstract</h5>
                                                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                                                      <span aria-hidden="true">&times;</span>
                                                    </button>
                                                  </div>
                                                  <div class="modal-body">
                                                    In this work, a new ensemble method for the task of category recognition in different environments is presented. The focus is on service robotic perception in an open environment, where the robot’s task is to recognize previously unseen objects of predefined categories, based on training on a public dataset. We propose an ensemble learning approach to be able to flexibly combine complementary sources of information (different state-of-the-art descriptors computed on color and depth images), based on a Markov Random Field (MRF). By exploiting its specific characteristics, the MRF ensemble method can also be executed as a Dynamic Classifier Selection (DCS) system. In the experiments, the committee- and topology-dependent performance boost of our ensemble is shown. Despite reduced computational costs and using less information, our strategy performs on the same level as common ensemble approaches. Finally, the impact of large differences between datasets is analyzed.
                                                  </div>
                                                  <div class="modal-footer font-italic">
                                                    Proceedings of the 9th Int. Conf. on Machine Vision (ICMV 2016), Nice, France, March 2017
                                                  </div>
                                                </div>
                                              </div>
                                            </div>
                                            <!--
                                            <a role="button" data-toggle="collapse" data-parent="#accordion" href="#collapse1" aria-expanded="false" aria-controls="collapse1">
                                                Active classifier selection for RGB-D object categorization using a Markov Random Field ensemble method.
                                            </a>
                                            <div id="collapse1" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="heading1">
                                                <div class="card card-body">
                                                    commented <div class="panel-body">
                                                    <p class="font-weight-bold mb-2"> Abstract: </p> </br>
                                                    <p class="mb-3">
                                                    ########ABSTRACT##########
                                                    </p>
                                                    <div class="font-italic">
                                                    Proceedings of the 9th Int. Conf. on Machine Vision (ICMV 2016), Nice, France, March 2017
                                                    </div>
                                                </div>
                                            </div>
                                            -->
                                            <!--
                                            <a href="#" onclick="return false;" role="button" class="btn-link" data-toggle="popover" title="abstract" data-content="...." placement="bottom" data-original-title="test title">Text to click</a>
                                            -->
                                            ICMV (<span class="font-weight-bold">Best Oral Presentation</span>)
                                        </div>
                                    <div>
                                        <a class="btn btn-outline-primary my-1 mr-1 btn-xsm" href="https://rmc.dlr.de/rm/de/staff/extcms/images/rmc/users/hillenbd/Durner_etal_16.pdf" target="_blank" rel="noopener">
                                        pdf
                                        </a>

                                        <button class = 'btn btn-default' data-toggle='modal' data-target='#citeModal' data-filecontents="cite.txt"> cite </button>
                                        <div id="citeModal" class="modal fade" tabindex="-1" role="dialog" aria-labelledby="citeModalLabel">
                                          <div class="modal-dialog modal-sm" role="document">
                                            <div class="modal-content">
                                            </div>
                                          </div>
                                        </div>

                                    </div>
                                    </div>
                                    <div class="flex-shrink-0">
                                        <span class="text-primary">2016</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

            </section>

            <button type="button" class="btn btn-primary" data-toggle="modal" data-target="#exampleModal" data-filename="cite.txt">Open modal for @getbootstrap</button>

            <div class="modal fade" id="exampleModal" tabindex="-1" role="dialog" aria-labelledby="exampleModalLabel">
              <div class="modal-dialog" role="document">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    <h4 class="modal-title" id="exampleModalLabel">New message</h4>
                  </div>
                  <div class="modal-body">
                    <form>
                      <div class="form-group">
                      <label for="recipient-name" class="col-form-label">Recipient:</label>
                        <input type="text" class="form-control" id="recipient-name">
                        <!--<object data="file.txt"></object>-->
                      </div>
                      <div class="form-group">
                        <label for="message-text" class="control-label">Message:</label>
                        <textarea class="form-control" id="message-text"></textarea>
                      </div>
                    </form>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
                    <button type="button" class="btn btn-primary">Send message</button>
                  </div>
                </div>
              </div>
            </div>
            <!-- Interests-->
            <!--
            <section class="resume-section" id="interests">
                <div class="resume-section-content">
                    <h2 class="mb-5">Interests</h2>
                    <p>Apart from being a web developer, I enjoy most of my time being outdoors. In the winter, I am an avid skier and novice ice climber. During the warmer months here in Colorado, I enjoy mountain biking, free climbing, and kayaking.</p>
                    <p class="mb-0">When forced indoors, I follow a number of sci-fi and fantasy genre movies and television shows, I am an aspiring chef, and I spend a large amount of my free time exploring the latest technology advancements in the front-end web development world.</p>
                </div>
            </section>
            <hr class="m-0" />
            -->
            <!-- Awards-->
            <!--
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Awards & Certifications</h2>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Google Analytics Certified Developer
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Mobile Web Specialist - Google Certification
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - University of Colorado Boulder - Emerging Tech Competition 2009
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - University of Colorado Boulder - Adobe Creative Jam 2008 (UI Design Category)
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - University of Colorado Boulder - Emerging Tech Competition 2008
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - James Buchanan High School - Hackathon 2006
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            3
                            <sup>rd</sup>
                            Place - James Buchanan High School - Hackathon 2005
                        </li>
                    </ul>
                </div>
            </section>
            -->
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
